Parameters:
  uploadBucketName:
    Type: String
    Description: The name of the Amazon S3 bucket where uploaded files will be stored.

Resources:
  s3bucketNotifications7E3FF01B:
    Type: Custom::S3BucketNotifications
    Properties:
      ServiceToken:
        Fn::GetAtt:
          - BucketNotificationsHandler050a0587b7544547bf325f094a3db8347ECC3691
          - Arn
      BucketName:
        Ref: s3bucketFBFA637E
      NotificationConfiguration:
        LambdaFunctionConfigurations:
          - Events:
              - s3:ObjectCreated:*
            Filter:
              Key:
                FilterRules:
                  - Name: suffix
                    Value: .csv
                  - Name: prefix
                    Value: row_data/
            LambdaFunctionArn:
              Fn::GetAtt:
                - lambdafunction45C982D3
                - Arn
      Managed: true
    DependsOn:
      - s3bucketAllowBucketNotificationsToCdkBlogV2Stacklambdafunction2ABA83354F1A497B
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/s3bucket/Notifications/Resource
  s3bucketFBFA637E:
    Type: AWS::S3::Bucket
    Properties:
      BucketName:
        Ref: uploadBucketName
    UpdateReplacePolicy: Retain
    DeletionPolicy: Retain
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/s3bucket/Resource
  s3bucketAllowBucketNotificationsToCdkBlogV2Stacklambdafunction2ABA83354F1A497B:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName:
        Fn::GetAtt:
          - lambdafunction45C982D3
          - Arn
      Principal: s3.amazonaws.com
      SourceAccount:
        Ref: AWS::AccountId
      SourceArn:
        Fn::GetAtt:
          - s3bucketFBFA637E
          - Arn
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/s3bucket/AllowBucketNotificationsToCdkBlogV2Stacklambdafunction2ABA8335
  Role1ABCC5F0:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
                - comprehend.amazonaws.com
                - glue.amazonaws.com
        Version: "2012-10-17"
      Description: allow lambda function to access s3 and Glue
      ManagedPolicyArns:
        - Fn::Join:
            - ""
            - - "arn:"
              - Ref: AWS::Partition
              - :iam::aws:policy/ComprehendFullAccess
      Policies:
        - PolicyDocument:
            Statement:
              - Action: iam:PassRole
                Effect: Allow
                Resource: !Sub "arn:aws:iam::${AWS::AccountId}:role/*"
            Version: "2012-10-17"
          PolicyName: "0"
        - PolicyDocument:
            Statement:
              - Action:
                  - glue:Update*
                  - glue:Create*
                  - glue:Get*
                  - glue:List*
                  - glue:Query*
                  - glue:Batch*
                  - glue:Check*
                  - glue:Search*
                  - glue:Start*
                  - glue:Delete*
                  - glue:Put*
                  - glue:Register*
                  - glue:Stop*
                  - glue:Delete*

                Effect: Allow
                Resource: "*"
            Version: "2012-10-17"
          PolicyName: "1"
        - PolicyDocument:
            Statement:
              - Action:
                  - logs:Create*
                  - logs:Describe*
                  - logs:List*
                  - logs:Filter*
                  - logs:Get*
                  - logs:Start*
                  - logs:Stop*
                  - logs:Test*
                  - logs:Associate*
                  - log:Cancel*
                  - log:Delete*
                  - log:Put*
                  - log:Update* 
                Effect: Allow
                Resource: "*"
            Version: "2012-10-17"
          PolicyName: "2"
        - PolicyDocument:
            Statement:
              - Action:
                  - s3:Get*
                  - s3:List*
                  - s3:Create*
                  - s3:Put*
                Effect: Allow
                Resource:
                  Fn::Join:
                    - ""
                    - - Fn::GetAtt:
                          - s3bucketFBFA637E
                          - Arn
                      - /*
            Version: "2012-10-17"
          PolicyName: "3"
        - PolicyDocument:
            Statement:
              - Action:
                  - s3:Get*
                  - s3:List*
                Effect: Allow
                Resource: "*"
            Version: "2012-10-17"
          PolicyName: "4"
      RoleName: Lambda-S3-Glue-comprehend
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/Role/Resource
  lambdafunction45C982D3:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import json
          import boto3
          import csv
          import boto3
          account_id = boto3.client("sts").get_caller_identity()["Account"]
          Glue_role_arn = 'arn:aws:iam::'+account_id+":role/Lambda-S3-Glue-comprehend" # replace with your IAM role created in step X
          client_glue = boto3.client('glue')
          iam = boto3.client('iam')
          s3 =boto3.client('s3')


          def main(event, context):
              bucket = event ['Records'][0]['s3']['bucket']['name']
              key = event ['Records'][0]['s3']['object']['key']
              #bucket ="gluecomprehendraafat-triger"
              #key ="row-data/"
              path = "s3://"+bucket+"/"+key
              key_list = key.split("/")
              key_prefix = key_list[0]

              key_list = key.split("/")
              table_name = key_list[1].split(".")[0]
              crawler_DB = table_name +"_DB"
              response_crawler_name_list = client_glue.list_crawlers(MaxResults=123)
              crawler_name_list = (response_crawler_name_list)['CrawlerNames']
              #check =  crawler_name in crawler_name_list
              check =  "glue_crawler_comprehend" in crawler_name_list
              if check is True:
                  response_delete_crawler = client_glue.delete_crawler(Name='glue_crawler_comprehend')


              ##########list DB and escape recreate DB if it's already created""#######
              response_db_list = client_glue.get_databases(CatalogId=account_id,ResourceShareType='ALL')
              DB_list = (response_db_list)['DatabaseList']
              db_name_list=[]
              db_index=0
              for db_name in DB_list:
                  db_name_list.append(DB_list[db_index]['Name'])
                  db_index += 1
              check_db =  crawler_DB.lower() in  db_name_list

              if check_db is False:
                  #creat DB for Glue
                  response_database = client_glue.create_database(
                  DatabaseInput={
                  'Name': crawler_DB,
                  'Description': 'glue_crawler_comprehend',
                  'LocationUri': 'string',
                  'Parameters': {'string': 'string'} })


              #creat Crawler
              table_prefix = table_name+"_"
              path_crawler = "s3://"+bucket+"/"+key_prefix
              response_crawler = client_glue.create_crawler(
              #Name=crawler_name,
              Name="glue_crawler_comprehend",
              Role=Glue_role_arn,
              DatabaseName=crawler_DB,
              Description='glue_crawler_comprehend',
              Targets={'S3Targets': [
                          {'Path': path_crawler},],} ,
                          TablePrefix=table_prefix,

                  )


              #run crawler
              #response = client_glue.start_crawler(Name=crawler_name)
              response = client_glue.start_crawler(Name= "glue_crawler_comprehend")
              #write the file name in text file to read it later in Glue job
              file_name = "file_name.txt"
              file_location = "file_location/" + file_name
              lambda_path = "/tmp/" + file_name
              s3_path = bucket+","+ key
              string = s3_path
              encoded_string = string.encode("utf-8")
              s3_resource = boto3.resource("s3")
              s3_resource.Bucket(bucket).put_object(Key=file_location, Body=encoded_string)
      Role:
        Fn::GetAtt:
          - Role1ABCC5F0
          - Arn
      FunctionName: trigger_data_cataloging
      Handler: index.main
      Runtime: python3.7
      Timeout: 240
    DependsOn:
      - Role1ABCC5F0
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/lambda_function/Resource
      aws:asset:path: asset.7456d1986d9b48653e7b6dfcf012fca317de97faa925777c61c51b5f91299917
      aws:asset:property: Code
  BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleB6FB88EC:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
        Version: "2012-10-17"
      ManagedPolicyArns:
        - Fn::Join:
            - ""
            - - "arn:"
              - Ref: AWS::Partition
              - :iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/BucketNotificationsHandler050a0587b7544547bf325f094a3db834/Role/Resource
  BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleDefaultPolicy2CF63D36:
    Type: AWS::IAM::Policy
    Properties:
      PolicyDocument:
        Statement:
          - Action: s3:PutBucketNotification
            Effect: Allow
            Resource: "*"
        Version: "2012-10-17"
      PolicyName: BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleDefaultPolicy2CF63D36
      Roles:
        - Ref: BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleB6FB88EC
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/BucketNotificationsHandler050a0587b7544547bf325f094a3db834/Role/DefaultPolicy/Resource
  BucketNotificationsHandler050a0587b7544547bf325f094a3db8347ECC3691:
    Type: AWS::Lambda::Function
    Properties:
      Description: AWS CloudFormation handler for "Custom::S3BucketNotifications" resources (@aws-cdk/aws-s3)
      Code:
        ZipFile: |
          import boto3  # type: ignore
          import json
          import logging
          import urllib.request

          s3 = boto3.client("s3")

          CONFIGURATION_TYPES = ["TopicConfigurations", "QueueConfigurations", "LambdaFunctionConfigurations"]

          def handler(event: dict, context):
              response_status = "SUCCESS"
              error_message = ""
              try:
                  props = event["ResourceProperties"]
                  bucket = props["BucketName"]
                  notification_configuration = props["NotificationConfiguration"]
                  request_type = event["RequestType"]
                  managed = props.get('Managed', 'true').lower() == 'true'
                  stack_id = event['StackId']

                  if managed:
                    config = handle_managed(request_type, notification_configuration)
                  else:
                    config = handle_unmanaged(bucket, stack_id, request_type, notification_configuration)

                  put_bucket_notification_configuration(bucket, config)
              except Exception as e:
                  logging.exception("Failed to put bucket notification configuration")
                  response_status = "FAILED"
                  error_message = f"Error: {str(e)}. "
              finally:
                  submit_response(event, context, response_status, error_message)


          def handle_managed(request_type, notification_configuration):
            if request_type == 'Delete':
              return {}
            return notification_configuration


          def handle_unmanaged(bucket, stack_id, request_type, notification_configuration):

            # find external notifications
            external_notifications = find_external_notifications(bucket, stack_id)

            # if delete, that's all we need
            if request_type == 'Delete':
              return external_notifications

            def with_id(notification):
              notification['Id'] = f"{stack_id}-{hash(json.dumps(notification, sort_keys=True))}"
              return notification

            # otherwise, merge external with incoming config and augment with id
            notifications = {}
            for t in CONFIGURATION_TYPES:
              external = external_notifications.get(t, [])
              incoming = [with_id(n) for n in notification_configuration.get(t, [])]
              notifications[t] = external + incoming
            return notifications


          def find_external_notifications(bucket, stack_id):
            existing_notifications = get_bucket_notification_configuration(bucket)
            external_notifications = {}
            for t in CONFIGURATION_TYPES:
              # if the notification was created by us, we know what id to expect
              # so we can filter by it.
              external_notifications[t] = [n for n in existing_notifications.get(t, []) if not n['Id'].startswith(f"{stack_id}-")]

            return external_notifications


          def get_bucket_notification_configuration(bucket):
            return s3.get_bucket_notification_configuration(Bucket=bucket)


          def put_bucket_notification_configuration(bucket, notification_configuration):
            s3.put_bucket_notification_configuration(Bucket=bucket, NotificationConfiguration=notification_configuration)


          def submit_response(event: dict, context, response_status: str, error_message: str):
              response_body = json.dumps(
                  {
                      "Status": response_status,
                      "Reason": f"{error_message}See the details in CloudWatch Log Stream: {context.log_stream_name}",
                      "PhysicalResourceId": event.get("PhysicalResourceId") or event["LogicalResourceId"],
                      "StackId": event["StackId"],
                      "RequestId": event["RequestId"],
                      "LogicalResourceId": event["LogicalResourceId"],
                      "NoEcho": False,
                  }
              ).encode("utf-8")
              headers = {"content-type": "", "content-length": str(len(response_body))}
              try:
                  req = urllib.request.Request(url=event["ResponseURL"], headers=headers, data=response_body, method="PUT")
                  with urllib.request.urlopen(req) as response:
                      print(response.read().decode("utf-8"))
                  print("Status code: " + response.reason)
              except Exception as e:
                  print("send(..) failed executing request.urlopen(..): " + str(e))
      Handler: index.handler
      Role:
        Fn::GetAtt:
          - BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleB6FB88EC
          - Arn
      Runtime: python3.8
      Timeout: 300
    DependsOn:
      - BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleDefaultPolicy2CF63D36
      - BucketNotificationsHandler050a0587b7544547bf325f094a3db834RoleB6FB88EC
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/BucketNotificationsHandler050a0587b7544547bf325f094a3db834/Resource
  Glue:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: pythonshell
        PythonVersion: "3"
        ScriptLocation: s3://aws-bigdata-blog/artifacts/simplify-data-discovery-for-business-users/blog/triger_glue_comprehend_workflow_v3.py
      Role:
        Fn::GetAtt:
          - Role1ABCC5F0
          - Arn
      DefaultArguments:
        --bucket_name:
          Ref: uploadBucketName
        --extra-py-file: s3://aws-bigdata-blog/artifacts/simplify-data-discovery-for-business-users/blog/python/library/boto3-1.17.70-py2.py3-none-any.whl
      Name: Glue_Comprehend_Job
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/Glue
  triggerdatadescAB8BE091:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          import json
          import os
          import logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          # Import Boto 3 for AWS Glue
          import boto3
          client = boto3.client('glue')

          # Variables for the job:
          glueJobName = "Glue_Comprehend_Job"

          # Define Lambda function
          def main(event, context):

              response = client.start_job_run(JobName = glueJobName)

              return response
      Role:
        Fn::GetAtt:
          - Role1ABCC5F0
          - Arn
      FunctionName: trigger_data_desc
      Handler: index.main
      Runtime: python3.7
      Timeout: 240
    DependsOn:
      - Role1ABCC5F0
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/trigger_data_desc/Resource
      aws:asset:path: asset.1873a2317d9c2cdae0f90439dbbc62905842e3b4c3f5cf1f20aaa32f3e129465
      aws:asset:property: Code
  ruleF2C1DCDC:
    Type: AWS::Events::Rule
    Properties:
      EventPattern:
        detail:
          crawlerName:
            - glue_crawler_comprehend
          state:
            - Succeeded
        detail-type:
          - Glue Crawler State Change
        source:
          - aws.glue
      State: ENABLED
      Targets:
        - Arn:
            Fn::GetAtt:
              - triggerdatadescAB8BE091
              - Arn
          Id: Target0
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/rule/Resource
  ruleAllowEventRuleCdkBlogV2Stacktriggerdatadesc033601A7980EBD10:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName:
        Fn::GetAtt:
          - triggerdatadescAB8BE091
          - Arn
      Principal: events.amazonaws.com
      SourceArn:
        Fn::GetAtt:
          - ruleF2C1DCDC
          - Arn
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/rule/AllowEventRuleCdkBlogV2Stacktriggerdatadesc033601A7
  CDKMetadata:
    Type: AWS::CDK::Metadata
    Properties:
      Analytics: v2:deflate64:H4sIAAAAAAAAE0WPwW7DIAyGn6V3x22Wy45bK+3QU5Q+ASVexhJAwrCqQrz7MO200/cb+ze/e+z7A+Z+96Zu3Ol53WftA2G+RKVXOH26UQVlKVKAk3ccQ9JRnidin4ImeGemWKcX4xZp1KnZRONdAVmZecB8THqlZnuqB46KqcCm7HVWmOUvCtYwVzN8JKdli5j+dAGjLObJb9QiCEe/GX1vQZsqwEOnJBNjiwb0Q64WeUpPW2UpBQ7/NwuXLRHW9tlfC4z3+OXdfsBXfNl9szFdSC4aSzg9+As6RaaCOAEAAA==
    Metadata:
      aws:cdk:path: CdkBlogV2Stack/CDKMetadata/Default
    Condition: CDKMetadataAvailable
Conditions:
  CDKMetadataAvailable:
    Fn::Or:
      - Fn::Or:
          - Fn::Equals:
              - Ref: AWS::Region
              - af-south-1
          - Fn::Equals:
              - Ref: AWS::Region
              - ap-east-1
          - Fn::Equals:
              - Ref: AWS::Region
              - ap-northeast-1
          - Fn::Equals:
              - Ref: AWS::Region
              - ap-northeast-2
          - Fn::Equals:
              - Ref: AWS::Region
              - ap-south-1
          - Fn::Equals:
              - Ref: AWS::Region
              - ap-southeast-1
          - Fn::Equals:
              - Ref: AWS::Region
              - ap-southeast-2
          - Fn::Equals:
              - Ref: AWS::Region
              - ca-central-1
          - Fn::Equals:
              - Ref: AWS::Region
              - cn-north-1
          - Fn::Equals:
              - Ref: AWS::Region
              - cn-northwest-1
      - Fn::Or:
          - Fn::Equals:
              - Ref: AWS::Region
              - eu-central-1
          - Fn::Equals:
              - Ref: AWS::Region
              - eu-north-1
          - Fn::Equals:
              - Ref: AWS::Region
              - eu-south-1
          - Fn::Equals:
              - Ref: AWS::Region
              - eu-west-1
          - Fn::Equals:
              - Ref: AWS::Region
              - eu-west-2
          - Fn::Equals:
              - Ref: AWS::Region
              - eu-west-3
          - Fn::Equals:
              - Ref: AWS::Region
              - me-south-1
          - Fn::Equals:
              - Ref: AWS::Region
              - sa-east-1
          - Fn::Equals:
              - Ref: AWS::Region
              - us-east-1
          - Fn::Equals:
              - Ref: AWS::Region
              - us-east-2
      - Fn::Or:
          - Fn::Equals:
              - Ref: AWS::Region
              - us-west-1
          - Fn::Equals:
              - Ref: AWS::Region
              - us-west-2
